{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m1mxEQIQqc-8"},"source":["# Feature Selection\n","\n","Notebook de resumo sobre os métodos de seleção de features"]},{"cell_type":"markdown","metadata":{},"source":["### Tipos de Algoritmos e Métodos\n","\n","Os métodos de seleção de features podem ser divididos basicamente em:\n","\n","![](https://miro.medium.com/max/1400/1*k8272idmlQ5X6JEuhwXGqA.png)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UvOee-2OvvbA"},"source":["- **Filter Methods**: Métodos de seleção de subconjuntos de atributos com base em seu relacionameno com a variável alvo.  Ele pode ser dividivo em:\n","    - Métodos estatísticos: utiliza medidas estatísticas para atribuir um score para cada feature, normalmente se usam testes univariados que consideram a independência da feature com a variável alvo. Exemplo: chi squared, scores com coeficiente de correlação.<br>\n","    - Feature Importance: As features são classificadas pelo score para serem mantidas ou removidas do modelo.  Métodos ensembles como o algoritmo Random Forest, podem ser usados para estimar a importância de cada atributo. Ele retorna um score para cada atributo, quanto maior o score, maior é a importância desse atributo.\n","<br>\n","<br>\n","\n","- **Wrapper Methods** : Métodos de seleção que selecionam um conjunto de features, onde diferentes combinações são preparadas, avaliadas e comparadas. Um modelo preditivo é usado para avaliar a combinação de features a atribuir um score baseado em uma acurácia de modelo. Exemplo: algoritmo RFE <br>\n","<br> \n","\n","- **Embedded Methods** : Métodos Embedded aprendem quais feature melhor contribuiem para a acurácia do modelo no momento de construção do modelo. Esse método combina os métodos Filter e o Wrapper, usando o que há de melhor em cada um. É implementado por algoritmos que têm seus próprios métodos de seleção de recursos internos. Dentre os métodos incorporados, os métodos mais usados são os de regularização, que penalizam um recurso, dado um limite de coeficiente.Exemplo: métodos de penalização, algoritmos Lasso, Elastic NEt e Ridge Regression.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"umigmSKjygvh"},"source":["### Carregando dados para demonstração\n","\n","Esses dados são somente para demonstração. \n","Valores podem estar errados em função do métodos aplicado."]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{},"colab_type":"code","id":"AtIFNL8hymsw"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{},"colab_type":"code","id":"EoriOHZD2pn4"},"outputs":[{"data":{"text/html":["<div><div id=03496358-3974-43ae-99eb-8faba31bdcbc style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('03496358-3974-43ae-99eb-8faba31bdcbc').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>preg</th>\n","      <th>plas</th>\n","      <th>pres</th>\n","      <th>skin</th>\n","      <th>test</th>\n","      <th>mass</th>\n","      <th>pedi</th>\n","      <th>age</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table></div>"],"text/plain":["   preg  plas  pres  skin  test  mass   pedi  age  class\n","0     6   148    72    35     0  33.6  0.627   50      1\n","1     1    85    66    29     0  26.6  0.351   31      0\n","2     8   183    64     0     0  23.3  0.672   32      1"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["colnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv', names=colnames)\n","df.head(3)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{},"colab_type":"code","id":"uRmpTny51bDf"},"outputs":[],"source":["X = df.drop(['class'], axis=1)\n","y = df['class']"]},{"cell_type":"markdown","metadata":{},"source":["## Filter Methods\n","\n","Os métodos de filtro são geralmente usados ​​como uma etapa de pré-processamento de dados. A seleção de recursos é independente de qualquer algoritmo de aprendizado de máquina. As características classificam com base em pontuações estatísticas que tendem a determinar a correlação das características com a variável de resultado. \n","\n","![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537552825/Image3_fqsh79.png)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2LA77RW0fyyV"},"source":["### Univariate Selection\n","\n","Podemos utilizar os métodos estatísticos tanto para classificação como para regressão, sendo os principais:\n","\n","- **f_classif**:  é adequado quando os dados são numéricos e a variável alvo é categórica.\n","- **mutual_info_classif**: indicado para quando *não* há uma dependência linear entre as features e a variável alvo.\n","- **f_regression**: aplicado para problemas de regressão.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Para utilizar esses classificadores podemos utilizar os métodos : SelectKBest e SelectPercentile"]},{"cell_type":"markdown","metadata":{},"source":["#### SelectKBest"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{},"colab_type":"code","id":"Osw0UoKCf4mb"},"outputs":[],"source":["# testendo o f_classif\n","from sklearn.feature_selection import SelectKBest # SelectKBest nos permite selecionar apenas os k melhores atributos\n","from sklearn.feature_selection import f_classif"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{},"colab_type":"code","id":"dVqJsmeyf4dc"},"outputs":[],"source":["# k=4, pedido para selecionar os 4 melhores atributos\n","# score_func -> passo aqui qual o método que quero aplicar, mas posso colocar somente o nome do metodo\n","\n","model1 = SelectKBest(score_func=f_classif, k=4)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{},"colab_type":"code","id":"EQdHlSJoQg4V"},"outputs":[{"data":{"text/plain":["SelectKBest(k=4)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# treinando o modelo com o f_classif\n","model1.fit(X,y)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{},"colab_type":"code","id":"uXY1zPHCv1-i"},"outputs":[],"source":["# aplicação do modelo nos dados de treino, para que seja feita a seleção dos melhores atributos\n","features = model1.transform(X)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"colab_type":"code","executionInfo":{"elapsed":1338,"status":"ok","timestamp":1583363241031,"user":{"displayName":"Rodrigo Santana Ferreira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4OVUnkCrXudTIiz5YNwndI2zAhcS1gWXUSvoqJQ=s64","userId":"14710330936720624047"},"user_tz":180},"id":"5kRaCzUFv7Lx","outputId":"efdf97ec-8e3d-4605-d6f4-2376ab8c9f32"},"outputs":[{"data":{"text/plain":["array([0, 1, 5, 7], dtype=int64)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# número das colunas que foram selecionadas\n","model1.get_support(indices=True)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["array(['preg', 'plas', 'mass', 'age'], dtype=object)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# nome das colunas que foram selecionadas\n","model1.get_feature_names_out(input_features=None)"]},{"cell_type":"markdown","metadata":{},"source":["A parte boa do SelectKBest é que ele funciona para problemas de regressão e classificação. \n","A parte ruim é que escolher o número K ideal é difícil. Muitas das vezes, o valor atribuído é bem empírico, mas você pode tentar contornar o SelectPercentile, que irá selecionar X% das melhores features."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hZoY4HY27yJ7"},"source":["#### SelectPercentile"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{},"colab_type":"code","id":"OjGuqYUd3yRS"},"outputs":[],"source":["from sklearn.feature_selection import SelectPercentile\n","from sklearn.feature_selection import chi2\n","\n","# chi2:  Mede a dependência entre variáveis estocásticas, o uso dessa função “elimina” \n","# os recursos com maior probabilidade de serem independentes da classe e, portanto,\n","# irrelevantes para a classificação."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{},"colab_type":"code","id":"OGttc1eI1avR"},"outputs":[{"name":"stdout","output_type":"stream","text":["número das colunas [1 4 5 7]\n","nome das colunas ['plas' 'test' 'mass' 'age']\n"]}],"source":["model2 = SelectPercentile(chi2, percentile=50) # 50% dos atributos mais relevantes\n","model2.fit(X, y)\n","features = model2.transform(X)\n","print(\"número das colunas\", model2.get_support(indices=True))\n","print(\"nome das colunas\", model2.get_feature_names_out(input_features=None))"]},{"cell_type":"markdown","metadata":{},"source":["Segundo o SKLearn \n","\n","- f_classif<br>\n","ANOVA F-value between label/feature for classification tasks.\n","<br> <br>\n","\n","- mutual_info_classif <br>\n","Mutual information for a discrete target.\n","<br><br>\n","\n","- chi2 <br>\n","Chi-squared stats of non-negative features for classification tasks.\n","<br><br>\n","\n","- f_regression <br>\n","F-value between label/feature for regression tasks | Coeficiente de Correlação de Pearson\n","<br><br>\n","\n","- mutual_info_regression <br>\n","Mutual information for a continuous target.\n","<br><br>\n","\n","\n","- SelectFpr <br>\n","Select features based on a false positive rate test.\n","<br><br>\n","\n","- SelectFdr <br>\n","Select features based on an estimated false discovery rate.\n","<br><br>\n","\n","- SelectFwe <br>\n","Select features based on family-wise error rate.\n","<br><br>\n","\n","- GenericUnivariateSelect <br>\n","Univariate feature selector with configurable mode.\n","\n","<br>\n","\n","\n","Segundo o SciPy\n","- tau de Kendall: kendalltau()\n","- Correlação de classificação de Spearman: spearmanr()\n","\n","\n","========================================================================================================\n","\n","![](https://i2.wp.com/dataaspirant.com/wp-content/uploads/2020/12/11-Univariate-Feature-Selection-Methods.png?resize=768%2C634&ssl=1)\n","\n","\n","Pela imagem da para ter uma ideia melhor sobre quando usar qual método:\n","\n","- Variável de entrada numérica e variável de saída numérica: Este é um problema de **regressão**.\n","    - Correlação de Pearson para uma relação linear.\n","    - Correlação de Spearman para um relacionamento monotônico.\n","<br><br>\n","\n","- Variável de Entrada Numérica e Variável de Saída Categórica: Este é um problema de **classificação**.\n","    - ANOVA para uma relação linear.\n","    - Kendall é para um relacionamento não linear.\n","<br><br>\n","\n","- Variável de entrada categórica e variável de saída numérica: Este é um problema de **regressão**. enfrentamos esse tipo de problema muito raramente.\n","    - ANOVA para uma relação linear.\n","    - Kendall é para um relacionamento não linear.\n","<br><br>\n","\n","- Variável de Entrada Categórica e Variável de Saída Categórica: Este é um problema de **classificação**.\n","    - Teste Qui-Quadrado.\n","    - Informação mútua.\n","\n","<br><br>\n","\n","Outra forma cmpreender os métodos é:\n","\n","![image](https://user-images.githubusercontent.com/71708626/156542691-7f3712fd-7d4f-4215-b3f8-b2bd9e94039b.png)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"havGsnwKAInl"},"source":["### Feature Importance\n","\n","Quando se trabalha com ensembles, como RandomForest e XGBoost, você pode contar com um aliado para identificar as features mais importantes de seu modelo: o atributo `feature_importance_`.\n","O `feature_importance_` irá retornar um array onde cada elemento dele é uma feature do seu modelo. Ele irá dizer, em proporções, quão importante aquela feature é para o modelo, onde quanto maior o valor, mais importante a feature é para o modelo. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"ZOfZg2KuAPtA"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":154},"colab_type":"code","executionInfo":{"elapsed":744,"status":"ok","timestamp":1583363843884,"user":{"displayName":"Rodrigo Santana Ferreira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4OVUnkCrXudTIiz5YNwndI2zAhcS1gWXUSvoqJQ=s64","userId":"14710330936720624047"},"user_tz":180},"id":"S2qcebNPAPq4","outputId":"65e4da66-e161-4550-9b88-2bffd6f03660"},"outputs":[{"data":{"text/plain":["RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=None, max_features='auto',\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=10,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)"]},"metadata":{},"output_type":"display_data"}],"source":["# feature extraction\n","model = RandomForestClassifier(n_estimators=10)\n","model.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"colab_type":"code","executionInfo":{"elapsed":845,"status":"ok","timestamp":1583363852258,"user":{"displayName":"Rodrigo Santana Ferreira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4OVUnkCrXudTIiz5YNwndI2zAhcS1gWXUSvoqJQ=s64","userId":"14710330936720624047"},"user_tz":180},"id":"ke1nQ3LVAPix","outputId":"d1ce0da4-c208-4271-d697-ae7885d6418d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.0805051  0.25383268 0.09511771 0.06774806 0.06528796 0.18677426\n"," 0.13028377 0.12045046]\n"]}],"source":["print(model.feature_importances_)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":675,"status":"ok","timestamp":1583363934825,"user":{"displayName":"Rodrigo Santana Ferreira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4OVUnkCrXudTIiz5YNwndI2zAhcS1gWXUSvoqJQ=s64","userId":"14710330936720624047"},"user_tz":180},"id":"JA9uZHrVuOwd","outputId":"ce420e13-5f23-429c-8ba1-e84ed2253f5c"},"outputs":[{"data":{"text/plain":["['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']"]},"metadata":{},"output_type":"display_data"}],"source":["colnames"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"ILbLBLfqulva"},"outputs":[],"source":["import pandas as pd\n","feature_importances = pd.DataFrame(model.feature_importances_,\n","                                   index = X.columns,\n","                                   columns=['importance']).sort_values('importance', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"colab_type":"code","executionInfo":{"elapsed":834,"status":"ok","timestamp":1583363977637,"user":{"displayName":"Rodrigo Santana Ferreira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4OVUnkCrXudTIiz5YNwndI2zAhcS1gWXUSvoqJQ=s64","userId":"14710330936720624047"},"user_tz":180},"id":"9VcHezCUuu6J","outputId":"a7351168-00a9-432c-e2c6-d15cb2ad1425"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>importance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>plas</th>\n","      <td>0.253833</td>\n","    </tr>\n","    <tr>\n","      <th>mass</th>\n","      <td>0.186774</td>\n","    </tr>\n","    <tr>\n","      <th>pedi</th>\n","      <td>0.130284</td>\n","    </tr>\n","    <tr>\n","      <th>age</th>\n","      <td>0.120450</td>\n","    </tr>\n","    <tr>\n","      <th>pres</th>\n","      <td>0.095118</td>\n","    </tr>\n","    <tr>\n","      <th>preg</th>\n","      <td>0.080505</td>\n","    </tr>\n","    <tr>\n","      <th>skin</th>\n","      <td>0.067748</td>\n","    </tr>\n","    <tr>\n","      <th>test</th>\n","      <td>0.065288</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      importance\n","plas    0.253833\n","mass    0.186774\n","pedi    0.130284\n","age     0.120450\n","pres    0.095118\n","preg    0.080505\n","skin    0.067748\n","test    0.065288"]},"metadata":{},"output_type":"display_data"}],"source":["feature_importances"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"colab_type":"code","executionInfo":{"elapsed":763,"status":"ok","timestamp":1583364006656,"user":{"displayName":"Rodrigo Santana Ferreira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4OVUnkCrXudTIiz5YNwndI2zAhcS1gWXUSvoqJQ=s64","userId":"14710330936720624047"},"user_tz":180},"id":"YCD0oUK5u9xy","outputId":"dda4b1ae-32fd-450d-a711-657989544320"},"outputs":[{"data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f5cd1394a58>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["feature_importances.plot(kind='bar')"]},{"cell_type":"markdown","metadata":{},"source":["-------------------"]},{"cell_type":"markdown","metadata":{},"source":["## Wrapper Methods\n","\n","O método wrapper precisa de um algoritmo de aprendizado de máquina e usa seu desempenho como critério de avaliação. É uma estratégia conhecida como __algoritmo guloso__, que é tentar atingir a melhor escolha tomando diversas pequenas boas escolhas\n","\n","![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537549832/Image2_ajaeo8.png)\n","\n","As principais forma de se aplicar os métodos Wrapper são: \n"," - RFE\n"," - SelectFromModel\n"," - SequentialFeatureSelector"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qhYj2nRO73Fa"},"source":["#### RFE - Recursive Feature Elimination\n","\n","- Com Recursive Feature Elimination é possivel construir modelos a partir da remoções de features.\n","\n","- Utiliza a acurácia do modelo para identificar atributos ou a combinação destes que melhor contribui para uma melhor performance.\n","\n","- Em grandes bases de dados o tempo de processamento pode ser um problema.\n","\n","\n","- Este método só funciona se o modelo tiver `coef_` ou `features_importances_`."]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{},"colab_type":"code","id":"ppryS2lg79f4"},"outputs":[],"source":["# Para a utilização do RFE é necessário importar o modelo e a função de seleção\n","\n","from sklearn.linear_model import LogisticRegression\n","model = LogisticRegression(max_iter=2000)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{},"colab_type":"code","id":"Xz62YdIo9hp4"},"outputs":[],"source":["from sklearn.feature_selection import RFE"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{},"colab_type":"code","id":"CBpWNQC07-SX"},"outputs":[],"source":["rfe = RFE(model, n_features_to_select=4, step=1)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{},"colab_type":"code","id":"fIBkmlOKRQZa"},"outputs":[],"source":["fit = rfe.fit(X, y)"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":655,"status":"ok","timestamp":1583363741639,"user":{"displayName":"Rodrigo Santana Ferreira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4OVUnkCrXudTIiz5YNwndI2zAhcS1gWXUSvoqJQ=s64","userId":"14710330936720624047"},"user_tz":180},"id":"htjvfxFmRk9m","outputId":"72c32ae4-66e9-4efa-f5cf-56002ae508d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["número das colunas [0 1 5 6]\n","nome das colunas ['preg' 'plas' 'mass' 'pedi']\n"]}],"source":["print(\"número das colunas\", rfe.get_support(indices=True))\n","print(\"nome das colunas\", rfe.get_feature_names_out(input_features=None))  "]},{"cell_type":"markdown","metadata":{},"source":["#### SelectFromModel\n","\n","Assim como o RFE, o SelectFromModel do Scikit-Learn é baseado em uma estimativa do modelo, para selecionar os recursos. As diferenças são que a seleção de recurso SelectFromModel é baseada no atributo de importância threshold (geralmente é `coef_` ou `feature_importances_` mas pode ser qualquer callable). Por padrão, o limite é a média."]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\sarah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"data":{"text/plain":["Index(['pedi'], dtype='object')"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_selection import SelectFromModel\n","\n","sfm_selector = SelectFromModel(estimator=LogisticRegression())\n","sfm_selector.fit(X, y)\n","X.columns[sfm_selector.get_support()]"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["número das colunas [6]\n","nome das colunas ['pedi']\n"]}],"source":["print(\"número das colunas\", sfm.get_support(indices=True))\n","print(\"nome das colunas\", sfm.get_feature_names_out(input_features=None))"]},{"cell_type":"markdown","metadata":{},"source":["#### SequentialFeatureSelector\n","\n","__Estratégia stepwise__ ou __regressão stepwise__ (visto que é mais comum de ver o uso dela com regressão linear na literatura de estatística), tem como objetivo comparar diferentes subconjuntos das features, escolhendo o melhor deles. \n","\n","A ideia é a seguinte:\n","- Imagina que comecemos usando todas as features. Temos então o quão bem o modelo performa.\n","- Agora, vamos comparar os modelos obtidos retirando 1 feature, e deixando todo o resto.\n","- O que for melhor, a gente coloca na lista final.\n","- Repetimos o processo, retirando 2 features, e assim por diante (3 features, 4 features, etc...).\n","- Escolhemos o melhor modelo dentre todos que estão na nossa lista final.\n","\n","O nome do processo acima é __backward stepwise selection__ (em uma tradução livre, \"seleção passo a passo de trás pra frente\"). É possível fazer o processo inverso também, __forward stepwise selection__ (\"seleção passo a passo seguindo pra frente\"), onde começamos com o mínimo de features, e vamos aumentando o número."]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"data":{"text/plain":["SequentialFeatureSelector(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n","                          direction='backward',\n","                          estimator=RandomForestClassifier(max_depth=5),\n","                          n_features_to_select=4, scoring='accuracy')"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import SequentialFeatureSelector\n","from sklearn.model_selection import KFold\n","\n","model = RandomForestClassifier(max_depth=5)\n","cv_splitter = KFold(n_splits=5, shuffle=True)\n","\n","feature_selector = SequentialFeatureSelector(model,\n","                                             n_features_to_select=4, \n","                                             direction='backward', \n","                                             scoring='accuracy',\n","                                             cv=cv_splitter)\n","\n","feature_selector.fit(X, y)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["número das colunas [1 2 5 7]\n","nome das colunas ['plas' 'pres' 'mass' 'age']\n"]}],"source":["print(\"número das colunas\", feature_selector.get_support(indices=True))\n","print(\"nome das colunas\", feature_selector.get_feature_names_out(input_features=None))"]},{"cell_type":"markdown","metadata":{},"source":["## Embedded methods\n","\n","Os métodos embutidos são iterativos no sentido de que cuidam de cada iteração do processo de treinamento do modelo e extraem cuidadosamente os recursos que mais contribuem para o treinamento de uma iteração específica. Os métodos de regularização são os métodos embutidos mais comumente usados que penalizam um recurso dado um limiar de coeficiente.\n","\n","É por isso que os métodos de regularização também são chamados de métodos de penalização que introduzem restrições adicionais na otimização de um algoritmo preditivo (como um algoritmo de regressão) que enviesam o modelo para menor complexidade (menos coeficientes).\n","\n","Exemplos de algoritmos de regularização são o LASSO, Elastic Net, Ridge Regression, etc.\n","\n","![](https://i0.wp.com/dataaspirant.com/wp-content/uploads/2020/12/7-Intrinsic-Feature-Selection-Method.png?resize=768%2C382&ssl=1)"]},{"cell_type":"markdown","metadata":{},"source":["- Regularização\n","Pode-se entender regularização como a inserção de bias em um modelo. Ou em outras palavras, essa técnica desencoraja o ajuste excessivo dos dados, afim de diminuir a sua variância.\n","Dentro da regressão linear, Ridge e Lasso são formas de regularizarmos a nossa função através de penalidades. De forma simples, dentro de uma equação estatística dos dados, nós alteramos os fatores de forma a priorizar ou não certas parcelas da equação e, assim, evitamos ‘overfitting’ e melhoramos a qualidade de predição.\n","<br><br>\n","- Seleção de Features\n","Na prática, o hiperparâmetro $\\alpha$, que controla a força da penalização, assume uma grande importância. Quando o $\\alpha$ é suficientemente grande, os coeficientes são forçados a ser exatamente iguais a zero, desta forma a dimensionalidade pode ser reduzida. Quanto maior for o parâmetro $\\alpha$, mais coeficientes serão encolhidos a zero. Por outro lado, se $\\alpha$ = 0, temos uma regressão linear comum."]},{"cell_type":"markdown","metadata":{},"source":["### LASSO - Least Absolute Shrinkage and Selection Operator (L1)\n","\n","É um método poderoso que executa principalmente duas tarefas: **Regularização** e **Seleção de Features**. \n","\n","Equação da regressão linear:\n","![](https://miro.medium.com/max/654/1*UFsN2JHfP5t_E1a9icTbwQ.png)\n","\n","Uma forma de diminuir o erro de overfit é aumentar o bias.\n","Para isso, regularizamos os coeficientes w, ou seja, restringimos o seu tamanho. \n","Isso é feito adicionando um termo na função de custo, de forma que minimizar a função de custo automaticamente minimize também os coeficientes.\n","\n","\n","Equação com a penalização LASSO:\n","![](https://miro.medium.com/max/1070/1*dynW6DLJxX2iaMrnfgxk_A.png)\n","\n","\n","Além de diminuir a variância do modelo, essa regularização tem uma outra importante aplicação em machine learning. \n","Quando há múltiplas features altamente correlacionadas (ou seja, features que se comportam da mesma maneira) a regularização Lasso seleciona apenas uma dessas features e zera os coeficientes das outras, de forma a minimizar a penalização L1.\n","Desse modo, dizemos que esse modelo realiza feature selection automaticamente, gerando vários coeficientes com peso zero, ou seja, que são ignorados pelo modelo. Isso facilita a interpretação do modelo, o que é uma enorme vantagem.\n"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"data":{"text/plain":["array(['preg', 'pres', 'skin', 'test', 'pedi', 'age'], dtype=object)"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import  GridSearchCV\n","from sklearn.linear_model import Lasso\n","\n","pipeline = Pipeline([\n","                     ('scaler',StandardScaler()),\n","                     ('model',Lasso())\n","])\n","\n","search = GridSearchCV(pipeline,\n","                      {'model__alpha':np.arange(0.1,10,0.1)},\n","                      cv = 5, scoring=\"neg_mean_squared_error\")\n","\n","search.fit(X,y)\n","\n","search.best_params_\n","\n","coefficients = search.best_estimator_.named_steps['model'].coef_\n","\n","importance = np.abs(coefficients)\n","importance\n","\n","features = X.columns\n","np.array(features)[importance > 0]\n","np.array(features)[importance == 0]"]},{"cell_type":"markdown","metadata":{},"source":["### Ridge (L2)\n","\n","Equação da regressão linear:\n","![](https://miro.medium.com/max/654/1*UFsN2JHfP5t_E1a9icTbwQ.png)\n","\n","Equação com penalização Ridge: \n","![](https://miro.medium.com/max/1060/1*_2_bFm9NkDrOmfOYs5lNFA.png)\n","\n","\n","\n","Nesse caso, a penalização consiste nos quadrados dos coeficientes, ao invés de seus módulos. \n","\n","Qual será o efeito dessa regularização nos coeficientes de duas features altamente correlacionadas?\n","Poderíamos ter duas features com coeficientes parecidos, ou uma com coeficiente alto, e outra com coeficiente zero. Como a penalização L2 é desproporcionalmente maior para coeficientes maiores, a regularização Ridge faz com que features correlacionadas tenham coeficientes parecidos.\n","No entanto, essa regularização não diminui a susceptibilidade do modelo a outliers, de forma que é recomendável limpar o dataset e remover features desnecessárias antes de realizar esse tipo de regressão.\n","\n","\n","SKLEARN:\n","- RidgeClassifier<br>\n","\n","\n","- RidgeCV<br>\n","Ridge regression with built-in cross validation.\n","\n","- KernelRidge<br>\n","Kernel ridge regression combines ridge regression with the kernel trick."]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean MAE: 0.337 (0.022)\n"]}],"source":["from sklearn.linear_model import Ridge\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from numpy import mean, std, absolute\n","\n","\n","# define model\n","model = Ridge(alpha=1.0)\n","# define model evaluation method\n","cv = KFold(n_splits=10)\n","# evaluate model\n","scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n","# force scores to be positive\n","scores = absolute(scores)\n","print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))\n"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE: -0.337\n","Config: {'alpha': 0.0}\n"]}],"source":["## Tuning Elastic Net Hyperparameters\n","from numpy import arange\n","from pandas import read_csv\n","from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n","from sklearn.model_selection import RepeatedKFold\n","from sklearn.linear_model import Ridge\n","\n","# define model\n","model = Ridge()\n","# define model evaluation method\n","cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n","# define grid\n","grid = dict()\n","grid['alpha'] = arange(0, 1, 0.01)\n","# define search\n","search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n","# perform the search\n","results = search.fit(X, y)\n","# summarize\n","print('MAE: %.3f' % results.best_score_)\n","print('Config: %s' % results.best_params_)"]},{"cell_type":"markdown","metadata":{},"source":["### Elastic Net — L1+L2\n","\n","Se trata exatamente de combinar os termos de regularização de L1 e L2. Assim, obtemos o melhor dos dois mundos, porém temos que enfrentar o problema de determinar dois hiperparâmetros para obter soluções ótimas.\n","\n","Equação da regressão linear:\n","![](https://miro.medium.com/max/654/1*UFsN2JHfP5t_E1a9icTbwQ.png)\n","\n","Equação com a penalização Elastic Net:\n","![](https://miro.medium.com/max/1386/1*elv_EM7V-1QvnVkD0Mmk8Q.png)\n","\n","\n","SKLEARN:\n","- ElasticNetCV<br>\n","Elastic net model with best model selection by cross-validation.\n","\n","- SGDRegressor<br>\n","Implements elastic net regression with incremental training.\n","\n","- SGDClassifier<br>\n","Implements logistic regression with elastic net penalty (SGDClassifier(loss=\"log\", penalty=\"elasticnet\"))."]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean MAE: 0.362 (0.018)\n"]}],"source":["from numpy import mean, std, absolute\n","from sklearn.model_selection import cross_val_score, RepeatedKFold\n","from sklearn.linear_model import ElasticNet\n","\n","# define model\n","model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n","# define model evaluation method\n","cv = RepeatedKFold(n_splits=10, n_repeats=3)\n","# evaluate model\n","scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n","# force scores to be positive\n","scores = absolute(scores)\n","print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"]},{"cell_type":"markdown","metadata":{},"source":["**Comparação entre LASSO, Ridge e Elastic Net**\n","\n","![](https://miro.medium.com/max/1400/1*PdC1TQDuQUepWpVLgswTmQ.png)\n","\n","No gráfico da esquerda, ainda que os pontos sejam aleatórios, observa-se uma tendência fraca nos dados (ruído). Essa tendência é capturada pela regressão linear e Ridge, mas não pela regressão Lasso ou Elastic Net. Para entender esse comportamento, observe que, nesse caso, os coeficientes da regressão linear são pequenos, de forma que a penalização Ridge, que conta com os coeficientes ao quadrado, é muito pequena. Já a penalização Lasso é alta o suficiente para levar os coeficientes a zero.\n","\n","\n","No gráfico do meio, a tendência de crescimento é maior, tal que a regressão Lasso obtém um coeficiente não nulo, ainda que pequeno. Os coeficientes da regressão Ridge são maiores e mais próximos do correto.\n","\n","\n","No gráfico da direta, a situação se inverte. As regressões Lasso e Elastic Net chegam muito próximo dos dados, mas a Ridge fica mais longe dos dados, porque a penalidade L2 é mais influenciada por coeficientes grandes.\n","\n","\n","Baseado nessas observações, podemos observar que a regressão com regularização Lasso tende a ignorar relações fracas, enquanto a regressão com regularização Ridge considera também relações fracas, mas não lida tão bem com relações fortes. A regressão Elastic Net é um intermediário entre Ridge e Lasso."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qct01oni3RON"},"source":["### Automatizando a Seleção de Features\n","\n","- Se estiver usando scikit-learn podemos usar Pipelines para automatização."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"OKiWyHtY4zDh"},"outputs":[],"source":["from sklearn.pipeline import Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"6Y9nVp573PRO"},"outputs":[],"source":["clf = Pipeline([\n","  ('feature_selection', RFE(LogisticRegression(max_iter=2000),4)),\n","  ('classification', RandomForestClassifier())\n","])"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":463},"colab_type":"code","executionInfo":{"elapsed":966,"status":"ok","timestamp":1583364243314,"user":{"displayName":"Rodrigo Santana Ferreira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4OVUnkCrXudTIiz5YNwndI2zAhcS1gWXUSvoqJQ=s64","userId":"14710330936720624047"},"user_tz":180},"id":"FfK7l11l3Fzx","outputId":"188d5eb6-bf4d-421c-af73-d56e32ee9097"},"outputs":[{"data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('feature_selection',\n","                 RFE(estimator=LogisticRegression(C=1.0, class_weight=None,\n","                                                  dual=False,\n","                                                  fit_intercept=True,\n","                                                  intercept_scaling=1,\n","                                                  l1_ratio=None, max_iter=2000,\n","                                                  multi_class='auto',\n","                                                  n_jobs=None, penalty='l2',\n","                                                  random_state=None,\n","                                                  solver='lbfgs', tol=0.0001,\n","                                                  verbose=0, warm_start=False),\n","                     n_features_to_select=4, step=1, verbose=0)),\n","                ('classi...\n","                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n","                                        class_weight=None, criterion='gini',\n","                                        max_depth=None, max_features='auto',\n","                                        max_leaf_nodes=None, max_samples=None,\n","                                        min_impurity_decrease=0.0,\n","                                        min_impurity_split=None,\n","                                        min_samples_leaf=1, min_samples_split=2,\n","                                        min_weight_fraction_leaf=0.0,\n","                                        n_estimators=100, n_jobs=None,\n","                                        oob_score=False, random_state=None,\n","                                        verbose=0, warm_start=False))],\n","         verbose=False)"]},"execution_count":35,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["clf.fit(X, y)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"colab_type":"code","executionInfo":{"elapsed":1319,"status":"ok","timestamp":1583364253108,"user":{"displayName":"Rodrigo Santana Ferreira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4OVUnkCrXudTIiz5YNwndI2zAhcS1gWXUSvoqJQ=s64","userId":"14710330936720624047"},"user_tz":180},"id":"j0OuEFR8TDEw","outputId":"3388b8c0-e718-4c4c-ba42-f86a2bfed220"},"outputs":[{"data":{"text/plain":["[('feature_selection',\n","  RFE(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n","                                   fit_intercept=True, intercept_scaling=1,\n","                                   l1_ratio=None, max_iter=2000,\n","                                   multi_class='auto', n_jobs=None, penalty='l2',\n","                                   random_state=None, solver='lbfgs', tol=0.0001,\n","                                   verbose=0, warm_start=False),\n","      n_features_to_select=4, step=1, verbose=0)),\n"," ('classification',\n","  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                         criterion='gini', max_depth=None, max_features='auto',\n","                         max_leaf_nodes=None, max_samples=None,\n","                         min_impurity_decrease=0.0, min_impurity_split=None,\n","                         min_samples_leaf=1, min_samples_split=2,\n","                         min_weight_fraction_leaf=0.0, n_estimators=100,\n","                         n_jobs=None, oob_score=False, random_state=None,\n","                         verbose=0, warm_start=False))]"]},"execution_count":36,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["clf.steps"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QyIp6Cw4rMJm"},"source":["**Refs**\n","\n","- https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n","- https://github.com/krishnaik06/Feature-Selection-techniques/blob/master/Feature%20Selection.ipynb\n","- https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n","- https://medium.com/analytics-vidhya/feature-selection-in-machine-learning-ec1f5d053007\n","- https://dataaspirant.com/feature-selection-methods-machine-learning/\n","- https://medium.com/analytics-vidhya/feature-selection-in-machine-learning-ec1f5d053007\n","- https://towardsdatascience.com/5-feature-selection-method-from-scikit-learn-you-should-know-ed4d116e4172\n","- https://medium.com/data-hackers/como-selecionar-as-melhores-features-para-seu-modelo-de-machine-learning-faf74e357913\n","- https://minerandodados.com.br/aprenda-como-selecionar-features-para-seu-modelo-de-machine-learning/\n","- https://medium.com/@airtonneto/sele%C3%A7%C3%A3o-de-atributos-para-data-science-e-machine-learning-2842c63fc59f\n","- https://www.datacamp.com/community/tutorials/feature-selection-python"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPkbAck+O03E0z90A//CeKm","collapsed_sections":[],"mount_file_id":"1_8d3o6at94EgSdQPtIDeAJ4g1dMTIxKU","name":"Seleção de features para Machine Learning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":0}
